{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b6ed14",
   "metadata": {},
   "source": [
    "# [Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives](https://mp.weixin.qq.com/s?__biz=MzU5ODQ5ODQ1Mw==&mid=2247608180&idx=1&sn=5c95c937e83429f70cb4eaf578f67efb&chksm=fe402a8ec937a398b6f5b9afb5ab3337871a09787a596a51a3a264c756fece669d2a3b9cb90d&scene=262&from=industrynews&version=4.0.19.6020&platform=win#rd)\n",
    "## 摘要\n",
    "&emsp;&emsp;智能汽车(IVs)凭借其增强的便利性、安全性优势和潜在的商业价值而受到广泛关注。尽管一些自动驾驶独角兽断言，到2025年，智能汽车将实现商业部署，但由于各种问题，它们的部署仍然局限于小规模验证，其中安全性、可靠性和规划方法的通用性是突出的问题。由于复杂环境下的感知缺陷，通过规划方法精确计算控制命令或轨迹仍然是智能汽车的先决条件，这对智能汽车的成功商业化构成了障碍。本文旨在回顾最先进的规划方法，包括pipeline规划和端到端规划方法。在pipeline方法方面，提供了对选择算法的调查，并讨论了扩展和优化机制，而在端到端方法中，驾驶任务的训练方法和验证场景是值得关注的问题。对实验平台进行了调查，以便于读者选择合适的培训和验证方法。最后，讨论了当前的挑战和未来的发展方向。本次调查中的并排比较有助于深入了解所审查方法的优势和局限性，这也有助于系统级的设计选择。\n",
    "## 引言\n",
    "&emsp;&emsp; 由于人工智能和计算机硬件的进步，智能汽车(IV)具有彻底改变交通的潜力，因此受到了政府、工业界、学术界和公众的广泛关注[2]。部署智能汽车具有减少道路交通事故和交通拥堵的巨大潜力，从而改善我们在过度拥挤的大城市中的机动性[3]。 尽管该领域的许多领先企业都做出了非凡的贡献，但由于对其可靠性和安全性的关键担忧，智能汽车仍然遥不可及，除非在有限的试验项目中。为了实现对周围环境的更高水平的感知，并提高安全性、效率和能力，智能汽车总是配备不同类型的传感器。尽管传感器种类繁多，但在不利场景下，智能汽车受检测能力不足的限制。因此，保证规划方法的安全性、鲁棒性和泛化性，是实现自动驾驶的关键问题[4]。  \n",
    "&emsp;&emsp; 本文对自动驾驶的总体规划方法进行了全面分析。 一般来说，自动驾驶的规划方法可以分为两类，即 pipeline 和端到端(end-to-end)。  \n",
    "&emsp;&emsp; pipeline规划方法，也称为基于规则的规划方法，是一种行之有效的方法，并被业界广泛采用。 如图 1a 所示，该方法是pipeline框架的一个子集，需要与其他方法结合才能完成自动驾驶任务，例如感知 [5]、定位和控制。作为一个主要优势，pipeline框架是可解释的，当发生故障或意外系统行为时，可以识别有缺陷的模块。为了限制第二部分的范围，我们只关注pipeline框架内的规划方法方面。 pipeline规划方法由两个主要部分组成：生成从起点到终点的道路级路径的全局路线规划，以及生成短期轨迹的局部行为和（或）轨迹规划。 虽然 pipeline 规划方法在工业上得到广泛应用，但该方法的局限性在于它需要大量的计算资源和许多人工启发式函数[6]。 在本研究中，我们关注pipeline规划方法的扩展和优化机制。  \n",
    "&emsp;&emsp; 端到端规划方法，也称为基于学习的方法，是一种新型方法，已成为自动驾驶汽车研究的一种趋势。 在这种方法中，整个驱动pipeline被视为一个单一的机器学习任务，它将原始感知数据转换为控制命令。 驾驶模型可以通过模仿学习学习知识，通过强化学习探索驾驶策略，通过并行学习不断自我优化。 尽管它的概念很吸引人，但很难甚至不可能找出模型行为不当的原因。 在这种情况下，我们的研究集中在端到端模型的网络结构、训练技术和部署任务上。\n",
    "&emsp;&emsp; 在第二节中，我们首先回顾了管道规划方法，包括全局路线规划和局部行为/轨迹规划，其中我们特别讨论了扩展和优化机制。 第三部分列出了经过审查的端到端规划方法，包括模仿学习、强化学习和并行学习。 详细探讨了网络架构、通用性和稳健性以及验证和确认方法。此外，大数据集、仿真平台和物理平台在智能和移动性更高的自动驾驶发展中起着辅助作用，因此，我们在第四节总结了自动驾驶的其他内容，包括数据集、仿真平台和物理平台。 最后，我们在第五节回顾了自动驾驶的当前挑战和未来方向。  \n",
    "&emsp;&emsp; 本文的主要贡献如下：  \n",
    "- 本文首次全面审查了IVs中的所有规划方法，包括模块化规划和端到端规划方法；\n",
    "- 本文提供了对最新数据集、模拟平台和半开放现实世界测试场景的全面分析和总结；\n",
    "- 本文总结了当前面临的挑战，并提出了未来的研究方向。\n",
    "## PIPELINE规划方法\n",
    "&emsp;&emsp; pipeline方法也被称为模块化方法，被工业界广泛使用，目前被认为是传统方法。pipeline系统源于主要为自主移动机器人发展而来的架构，这些架构由独立但相互连接的模块构建，例如感知、定位、规划和控制。  \n",
    "&emsp;&emsp; pipeline方法规划框架负责计算ego车辆的低级别控制器要跟踪的轨迹点序列，通常包含三个功能，即全局路线规划、局部行为规划和局部轨迹规划。全局路线规划在全局地图上提供从起点到终点的道路级路径。  \n",
    "&emsp;&emsp; “局部”代表在空间或时间范围内较短的合成轨迹，否则ego车辆无法对超出传感器范围的风险做出反应，局部行为规划决定接下来几秒钟的驾驶行为类型（例如，跟车、nudge、side pass、让行和超车），同时局部轨迹规划基于决定的行为类型生成短期轨迹。  \n",
    "&emsp;&emsp; 实际上，局部行为规划和局部轨迹规划之间的界限相当模糊，例如，一些行为规划者所做的不仅仅是识别行为类型。 为便于理解，本文不严格区分这两种功能，将相关方法简单视为轨迹规划方法。 因此，本节将相关算法分为两个功能：全局路线规划和局部行为/轨迹规划。  \n",
    "### 全局路径规划\n",
    "&emsp;&emsp; 全局路径规划负责在道路网络中找到最佳道路级路径，该路径以包含数百万条边和节点的有向图的形式呈现。路径规划器在有向图中搜索，以找到连接起点和终点节点的最小成本序列。在此，成本是基于所考虑的查询时间、预处理复杂性、内存占用率和解决方案鲁棒性来定义的[8]。Edsger Wybe Dijkstra是这一领域的先驱，他创新性地提出了以他的名字命名的Dijkstra算法[9]。Lotfi等人[10]构建了一个基于Dijkstra的智能调度框架，该框架计算每个代理的最优调度，包括最大速度、最小移动和最小消耗成本。Astar算法[11]是道路级导航任务中另一种著名的算法，它利用启发式函数的优势来精简研究空间。所有这些算法都大大缓解了交通效率问题，并在智能交通系统领域引起了极大的关注。\n",
    "### 局部行为/轨迹规划\n",
    "&emsp;&emsp; 局部行为规划和局部轨迹规划功能协同工作，基于路线规划中计算的全局路线来计算一条安全、舒适和连续的局部轨迹。由于产生的轨迹是局部的，除非全局目的地不远，否则这两个功能必须以receding-horizon的方式实现[12]。值得强调的是，这两个功能的输出应该是轨迹，而不是路径[13,14]，并且轨迹与其他动态交通参与者相互作用，否则，自车需要额外的努力来躲避环境中的移动障碍物。\n",
    "&emsp;&emsp; 名义上，局部规划是通过解决最优控制问题（OCP）来完成的，该问题最小化预定义的成本函数，同时满足多种类型的硬约束或软约束[15]，[16]。OCP 的解决方案呈现为时间连续控制和状态配置文件，其中所需轨迹由状态配置文件的一部分反映。 由于此类 OCP 的解析解通常不可用，因此需要两种类型的操作来构建轨迹。具体来说，局部规划分为三部分，第一步是识别一系列状态网格，第二步是在相邻状态网格之间生成primitive，第三步是前两者的有机结合。\n",
    "#### 状态网格识别\n",
    "&emsp;&emsp; 状态网格识别可以通过搜索、选择、优化或潜在最小化来完成。基于搜索的方法将与前述OCP相关的连续状态空间抽象成图，并在那里找到状态的链接。流行的基于**搜索**的方法包括Astar搜索[17]和动态规划（DP）[17, 18]。这些算法的许多高级应用已经将其影响力推到了顶峰，如混合Astar[19]、双向Astar、半优化Astar[20]和LQG框架[18]。基于**选择**的方法通过寻找具有最优成本函数的候选者，决定下一步或多步的状态网格。贪婪选择[21]和马尔可夫决策过程（MDP）系列方法通常[22, 23]属于这一类。  \n",
    "&emsp;&emsp; 基于**优化**的方法将原始OCP离散化为数学规划(MP)，其解为高分辨率状态网格[24,25]。MP解算器进一步分为基于梯度的解算器和非基于梯度的求解器；基于梯度的求解器通常求解非线性规划[16]、二次规划[24]、[26，27]、二次约束二次规划[28]和混合整数规划；基于非梯度的解算器通常由元启发式表示[29]。结合多种前述方法可以提供由粗到细的局部行为/运动规划策略。  \n",
    "#### primitive生成\n",
    "&emsp;&emsp; Primitive生成通常有封闭形式规则、模拟、插值和优化。**封闭形式规则**是通过具有封闭形式解的分析方法来计算primitive的方法。典型的方法包括Dubins/Reed-Shepp曲线[30,31]、多项式[21]和理论最优控制方法[32,33]。基于**仿真**的方法通过forwarding仿真生成轨迹/路径primitive，由于没有自由度，因此运行速度很快[17]。基于**插值**的方法由样条曲线或参数化多项式表示[31]。基于**优化**的方法通过数值方式求解小规模 OCP 以连接两个状态网格[34,35]。\n",
    "#### 其他方法\n",
    "&emsp;&emsp; 状态网格识别和primitive 生成是构建轨迹的两个基本操作。这两种操作都可以以各种方式进行组织。例如，Kuwata等人[36]将两种操作集成在一个迭代循环中；HU等人[34]在在线状态网格识别之前，离线构建primitives图；Fan等人[27]在生成连接primitives之前，识别状态网格。如果规划者只找到一条路径而不是一条轨迹，那么时间进程应该被附加到计划的路径上，作为后处理步骤[35]。这种策略，被称为路径速度分解（PVD），被广泛使用，是因为它将一个3D问题转换为两个2D问题，这在很大程度上促进了解决过程。相反，非PVD方法直接规划轨迹，这具有提高最优解的潜在优点[18]、[37]-[39]。\n",
    "\n",
    "&emsp;&emsp; 该研究领域的最新研究包括如何开发适合特定场景/任务的特定规划者，特别是[12]、[38]，以及如何在上游/下游模块不完善的情况下规划安全轨迹[38]。在过去的几十年里，自动驾驶领域取得了越来越快的进步。除了计算硬件的进步外，移动机器人运动规划理论计算方面的重大理论进展也促成了这种快速进步。毫无疑问，智能汽车将提供更好的道路网络利用率和安全性，从而推动了研究工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56006b6",
   "metadata": {},
   "source": [
    "## 端到端的规划方法\n",
    "&emsp;&emsp; 端到端是从原始传感器数据到轨迹点或控制信号的直接映射。由于它能够提取特定任务的策略，因此在各个领域都取得了巨大成功。与pipeline方法相比，感知和控制模块之间没有外部间隙，很少嵌入人工定制的启发式算法，因此端到端方法更有效地处理车辆与环境的交互[40]。端到端有更高的天花板，有潜力在自动驾驶领域达到专家级的表现。本节中将端到端方法分为三类：模仿学习、强化学习和并行学习。  \n",
    "### 模仿学习  \n",
    "&emsp;&emsp; 模仿学习(Imitation learning, IL)是指基于专家轨迹的智能体学习策略，通常提供专家决策和控制信息[41]。每个专家轨迹都包含一系列状态和动作，并提取所有“状态-动作”对来构建数据集。在IL任务中，模型利用构建的数据集来学习状态和动作之间的潜在关系，状态对应特征，动作对应标签。因此，IL的具体目标是评估状态和动作之间的最适合度映射，以便智能体尽可能达到专家轨迹。[表1](#table1)列出了本部分复习的所有著名的模仿学习方法。  \n",
    "&emsp;&emsp; 广泛使用的训练方法有3种[60]，第一种表现为负向方法，命名为行为克隆（BC）； 第二种建立在 BC 之上，称为直接策略学习（DPL）； 最后一种是依赖于任务的方法，称为逆向强化学习（IRL）方法。  \n",
    "\n",
    "表1<span id='table1'></span>\n",
    "\n",
    "Article | Category | Input | Output | Implement Tasks | Auxiliary Method | Dataset\n",
    ":- | :- | :- | :- | :- | :- | :-\n",
    "Bojarski et al. [42] | BC | monocular image | steering angle | lane Keeping | CNN is the only component of end-to-end model | physical & simulate platform\n",
    "Codevilla et al. [43] | BC | monocular image | control information | simulation navigation task | High-level commands as a switch to select the branch | Carla\n",
    "Chen et al. [44] | BC | monocular image | control information | simulation navigation task | Affordance is used to predict control actions | TORCS Dataset & KITTI\n",
    "Sauer et al. [45] | BC | monocular video & directional input | control information | physical navigation task | Conditional affordance is trained to calculate intermediate representations | Carla\n",
    "Zeng et al. [46] | BC | Lidar data & HD Map | trajectory, scenario representation | physical navigation task | The intermediate representation is used to improve the model’s interpretability | physical dataset collected in North America\n",
    "Sadat et al. [47] | BC | Lidar data & HD Map | trajectory, scenario representation | physical navigation task | A joint system with interpretable intermediate representations for interpretable planner | physical dataset collected in North America\n",
    "Ross et al. [48] | DPL | monocular image | control information | autonomous racing competition | An iterative algorithm is proposed to guarantee the performance in corner cases | 3D racing simulator\n",
    "Zhang et al. [49] | DPL | monocular image | control information | autonomous racing competition | Embedded query-efficient model to reduce the requirement for expert trajectories | racing car simulator\n",
    "Yan et al. [50] | DPL | LiDAR, ego-vehicle speed, Sub-goal | control information | physical & simulation navigation task | The novice and the expert policy is fused to control the robot | physical and simulate platform\n",
    "Li et al. [51] | DPL | monocular image & sub-goal | waypoint, control information | autonomous racing | A reward-based online method learns from multiple experts | Sim4CV\n",
    "Ohn-Bar et al. [52] | IRL | monocular image | control information | simulation navigation task | Scenario context is embedded into the policy learning network | Carla\n",
    "Levine et al. [53] | IRL | BEV image | control information | keep the lane, **change lanes** & takeover | The gaussian algorithm is used to learn the relevance of features in expert trajectories. | Highway driving simulator\n",
    "Brown et al. [54] | IRL | monocular image | control information | keep the lane, **change lanes** & takeover | The high-confidence upper bounds on the alpha-worstcase are embedded into the policy network. | Highway driving simulator\n",
    "Palan et al. [55] | IRL | monocular image | control information | keep the lane, **change lanes** & takeover | A globally normalized reward function is constructed. | Lunar lander simulator\n",
    "Ziebart et al. [56] | IRL | Road network, Sub-goal, & GPS Data | control information | long range autonomous navigation task | A probabilistic approach is proposed for maximum entropy | Driver route modeling\n",
    "Lee et al. [57] | IRL | monocular image | control information, costmap | keep the lane, **change lanes** & takeover | The query generation process is used to improve the generalization | NGSIM & Carla\n",
    "Ho et al. [58] | IRL | monocular image | control information | keep the lane, **change lanes** & takeover | GAN is integrated into the end-to-end model | Carla\n",
    "Phan et al. [59] | IRL | BEV image, HD map, obstacle information | Control information physical navigation task | A three-step IRL planner is proposed | physical dataset from the Las Vegas Strip\n",
    "\n",
    "#### 行为克隆(Behavioral Cloning, BC)\n",
    "&emsp;&emsp;行为克隆(Behavioral Cloning, BC)是自动驾驶中IL的主要方法[42],[61]。智能体利用从专家到训练模型的状态-动作对，然后使用分类器/回归器复制策略。BC是一种被动方法，其目标是通过被动观察命令的完整执行来学习目标策略，然而，这需要所有轨迹中的状态-动作对是独立的前提。Bojarski等人[45]为BC构建了一个开创性的框架，该框架训练卷积神经网络仅计算前视图单眼相机的转向。这种方法只输出横向控制，而忽略纵向命令，因此只能在有限数量的简单场景中实现。Codevilla等人[46]提出了一个著名的IL模型，称为条件模仿学习（CIL），该模型包含横向和纵向控制，如图2所示。单目图像、ego车辆的速度测量和高级命令（直行、左行、右行和车道跟随）被用作CIL的输入，输出预测的纵向和横向控制命令。每个命令都作为一个开关来选择一个专门的子模块。CIL是CL方法在自动驾驶中的一个里程碑，它证明了卷积神经网络（CNN）可以学习自主执行车道和道路跟踪任务。  \n",
    "&emsp;&emsp;基于CIL[43]，许多研究人员在输入阶段包括了额外的信息，如全局路线、位置信息或点云[62]-[64]。由于有足够的感知数据输入，这些方法在各种条件下都表现出较强的泛化能力和鲁棒性。  \n",
    "&emsp;&emsp;由于其新颖的结构，IL方法排除了不同子系统之间的不确定性估计，并减少了反馈毫秒。然而，这一特征导致了一个显著的缺点，即缺乏可解释性，无法提供足够的理由来解释决策。许多研究人员试图通过插入intermediate representation层来解决这个难点。Chen等人[44]提出了一种新的范式，称为直接感知方法，用于预测城市自动驾驶场景的affordance。affordance是一种 BEV 格式，该格式清楚地显示周围环境的特征，然后被馈送到低级别控制器以产生转向和加速度。Sauer等人[45]进一步提出了一种改进的直接感知模型，该模型利用视频和高级命令到intermediate representations，并计算控制信号，将其作为输出。与[44]相比，该模型可以处理城市交通场景中的复杂场景。Urtasun和她的团队还提出了两个可解释的端到端规划器[46，47]，这两个规划器都利用原始激光雷达数据和高清地图（HD Map）来预测安全轨迹和intermediate representations，这些表示用于呈现政策如何回应周围的场景。  \n",
    "&emsp;&emsp;BC方法的主要特点是**只有专家才能生成训练示例，这直接导致训练集是在学习策略执行过程中访问的状态的子集[68]。因此，当数据集存在偏差或过度拟合时，该方法仅限于泛化。此外，当智能体被引导到未知状态时，很难学习正确的恢复行为**。  \n",
    "#### 直接策略学习(Direct Policy Learning, DPL)\n",
    "&emsp;&emsp;直接策略学习(Direct Policy Learning, DPL)是一种基于BC的训练方法，它评估当前策略，然后获得更合适的训练数据进行自优化。与BC相比，DPL的主要优势是利用专家轨迹来指导智能体如何从当前错误中恢复[60]。通过这种方式，DPL缓解了由于数据不足而造成的BC限制。本节总结了一系列DPL方法。  \n",
    "&emsp;&emsp;Ross等人[48]构建了一种经典的在线IL方法，称为数据集聚合（DAgger）方法。这是一种基于Follow-the-Leader算法[60]的主动方法，每次验证迭代都是一个在线学习例子。该方法修改了智能体所经历的所有状态-动作对的主分类器或回归器。DAgger是解决序列预测问题的一种新方法，但它的学习效率可能会受到策略空间和学习空间之间距离的影响。针对这个问题，He等人[66]提出了一种基于coaching算法的DAgger，该算法雇佣教练为学习者演示易学习的策略，并且演示的策略逐渐收敛到标签。为了更好地指导智能体，教练建立了一个折中策略，该策略并不比真实控制信号差太多，但比新手预测的行动好得多。\n",
    "如图3所示，$\\pi$是预测命令，$\\pi^*$表示专家轨迹，$\\pi^{'}$表示折中轨迹。对于智能体，在每次迭代中学习次优策略$\\pi^{'}$比$\\pi^*$ 更加容易。该策略是近似最优的。  \n",
    "&emsp;&emsp;其他研究人员也指出了DAgger方法的一些缺点[48]、[66]：查询效率低，数据收集不准确，泛化能力差。针对这一问题，Zhang等人[52]提出了SafeDAgger算法，旨在提高DAgger的查询效率，并可以进一步降低对标签准确性的依赖。Hoque等人[67]提出了一个ThriftyDAgger模型，该模型集成了人类对极端情况的反馈；Yan等人[53]针对无地图场景中的导航任务，提出了一种新的DPL训练方案，这两种方案都提高了模型的泛化能力和鲁棒性。  \n",
    "&emsp;&emsp;基于 DAgger 的方法减少了对数据集的依赖性并提高了学习效率，但是，这些方法无法区分专家轨迹的好与坏，并且忽略了不适应行为的学习机会。 针对这一问题，Li等人[51]提出了观察模仿学习（OIL）方法，该方法从单目图像预测控制命令，并将途径点编码为intermediate\n",
    "representations。 OIL是一种基于奖励函数的在线学习策略，它可以向多位专家学习并放弃错误的策略。  \n",
    "&emsp;&emsp;为了微调感知到行动方法中的智能体策略，Ohn-Bar等人[52]提出了一种优化情景驾驶策略的方法，该方法可以有效地捕捉不同场景中的推理，如图4所示。训练分为三个部分。首先该模型通过BC方法学习次优策略。其次上下文编码被训练，以学习 scenario 特征。第三，通过与仿真的在线交互来完善集成模型，并通过基于DAgger的方法收集更好的数据。  \n",
    "&emsp;&emsp;DPL是一种迭代的在线学习策略，它减轻了对数据集数量和分布的要求，同时通过有效地消除不正确的策略，促进策略的持续改进。  \n",
    "#### 反向强化学习(Inverse Reinforcement Learning, IRL) \n",
    "&emsp;&emsp;反向强化学习(Inverse Reinforcement Learning, IRL)旨在通过推理输入和输出之间的潜在原因来规避上述方法的缺点。与之前的方法相似，IRL需要在开始时收集一组专家轨迹。然而，该方法首先推理这些专家轨迹，然后基于缜密的奖励函数优化行为策略，不再是简单地学习状态-动作映射。IRL方法可以分为三类，最大margin方法、贝叶斯方法和最大熵方法。  \n",
    "\n",
    "&emsp;&emsp; 1. 最大margin方法利用专家轨迹来评估奖励函数，该函数最大化最优策略和估计次最优策略之间的margin。这些方法使用线性组合算法表示具有一组特征的奖励函数，其中所有特征都被认为是独立的。  \n",
    "&emsp;&emsp;Andrew Wu[68]是该领域的先驱，他首次引入最大margin IRL方法，他提出了三种计算精制的奖励函数的算法。此外，Pieter等人[69]在[68]的基础上设计了一种优化算法，该算法假设专家奖励函数可以表示为已知特征的人工线性组合，目的是揭示权重和特征之间的潜在关系。  \n",
    "&emsp;&emsp;上述方法的局限性在于，专家轨迹的质量和分布限定了该方法的性能上限。针对这一问题，Umar等人[70]提出了一种基于博弈论的IRL方法，称为multiplicative weights for apprenticeship learning，它能够向代理导入关于每个特征权重的先验策略，并利用线性规划算法来修改奖励函数，使其策略是稳定的。  \n",
    "&emsp;&emsp;此外，Phan Minh等人[59]提出了一个可解释的规划系统，如图5所示。轨迹生成模块利用感知信息来计算一组未来轨迹。安全过滤器通过可解释的方法，保证基本安全。DeepIRL预测轨迹评分，这是该系统的核心贡献。此外，[71]和[72]提出了偏好推理公式，用户可以根据个人偏好选择动作，这确实提高了模型的性能。  \n",
    "    \n",
    "&emsp;&emsp; 2. IRL的第二部分是贝叶斯方法，它通常利用优化轨迹或者奖励的先验分布来最大化奖励的后验分布。Ramachandran等人[73]提出了第一个贝叶斯IRL。从贝叶斯的角度参考了IRL模型，并从先验分布推理出估计奖励函数的后验分布。Levine等人[53]将核函数集成到贝叶斯IRL模型[73]中，以提高估计奖励的准确性，并提高不可见驾驶的性能。  \n",
    "&emsp;&emsp; 此外，Brown 等人[54]构建了一个基于抽样的贝叶斯 IRL 模型，该模型在无奖励功能的不可见的场景下，利用专家轨迹，计算预期回报中$\\alpha$最坏情况差异的实际高置信度上限。Palan 等[55] 提出 DemPref 模型，该模型利用专家轨迹学习粗略的奖励函数，该轨迹用于确定（主动）查询生成过程，以提高生成查询的质量。DemPref 缓解了标准的基于偏好的学习方法所面临的效率问题，并且不完全依赖于高质量的专家轨迹。  \n",
    "    \n",
    "&emsp;&emsp; 3. IRL的第三部分是最大熵方法，它通过使用优化中的最大熵估计奖励函数来定义。与以前的IRL方法相比，最大熵方法更适合连续空间，并且具有解决专家轨迹次优影响的潜在能力。Ziebart[56]提出了第一最大熵IRL模型，该模型使用与[68]相同的方法，可以缓解专家轨迹中的噪声和不完美行为。智能体试图通过将特征线性地映射到奖励，优化监督下的奖励函数。  \n",
    "&emsp;&emsp; 然后，许多研究人员[57]、[58]、[74]将最大熵IRL应用到物理端到端自动驾驶。其中，[58]提出了**生成对抗模仿学习（GAIL），该算法已成为该领域的经典算法**。GAIL利用生成对抗性网络（GAN）以无模型方法生成专家轨迹的分布，以缓解由数据集不足所导致的状态漂移问题。由于充足的重建专家轨迹和竞争政策，GAIL在特定场景下达到与人类驾驶员相当的性能。基于[58]，许多工作被提出，如InfoGAIL[75]、Directed-InfoGAIL[76]、Co GAIL[77]，它们都在各自的实现领域取得了有竞争力的成果。    \n",
    "&emsp;&emsp; IRL为自动驾驶提供了几项出色的工作，然而，与上述方法一样，**它在极端情况下也存在长尾问题**。如何有效地提高IRL的稳健性和可解释性也是未来的方向。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc47845",
   "metadata": {},
   "source": [
    "### 强化学习\n",
    "&emsp;&emsp;**模仿学习方法需要大量人工标注数据，不同的驾驶员在遇到完全相同的情况时，可能会做出完全不同的决定，这会导致训练过程中出现不确定性困境**。为了避免对标注数据的依赖，一些研究人员努力利用**强化学习(Reinforcement Learning, RL)算法**进行自主决策规划。智能体可以通过与环境交互来获得一些奖励。RL的目标是通过反复试验来优化累积数值奖励。通过持续与环境持续交互，智能体逐渐获得最佳策略的知识，以达到目标端点。  \n",
    "&emsp;&emsp;随着人工智能的进步，深度强化学习（DRL）集成深度学习的特征提取能力与传统强化学习的决策能力。这有助于解决高维状态和大规模的行动空间带来的困境，并最终实现从状态输入到行动输出的端到端自动驾驶。在本文中，我们将主要的强化学习方法分为四类：基于价值的强化学习、基于策略的强化学习，层次强化学习（HRL）和多智能体强化学习（MARL），[表2](#table2)列出了回顾的方法。\n",
    "\n",
    "表2<span id='table2'></span>\n",
    "\n",
    "Article | Method | Observation | Output | Scenario | Simulator\n",
    ":- | :- | :- | :- | :- | :-\n",
    "Wolf et al. [81] | Value-based, DQN | front cam | discrete Steering angle | lane keeping | Gazebo\n",
    "Alizadeh et al. [84] | Value-based, DQN | relative distance & velocity value | trajectory points | lane change | Self-made environment\n",
    "Ronecker et al. [85] | Value-based, DQN | relative distance & velocity value | trajectory points | lane change, highway strategy | Self-made environment\n",
    "Li et al. [89] | Value-based, DQN | front cam | discrete lane change action | lane change & city strategy | CARLA\n",
    "Mo et al. [93] | Value-based, DQN | front cam | discrete acceleration & lane change action | overtakeing & highway strategy | SUMO\n",
    "Kendall et al. [94] | Policy-based, DDPG | front cam | continuous steering angle & speed setpoint | lane keeping | Unreal Engine 4\n",
    "Wang et al. [96] | Policy-based, DDPG, DQN | front cam | discrete lane change action | lane change | Self-made environment\n",
    "Saxen et al. [97] | Policy-based, PPO | lane based grid | continuous acceleration & steering angle | highway kinematic | Open source simulator\n",
    "Ye et al. [99] | Policy-based, PPO | relative distance & velocity | discrete lane change action | lane change | SUMO\n",
    "Liang et al. [102] | Policy-based, DDPG | front cam, Speed | continuous steering angle, acceleration, braking | navigation | CARLA\n",
    "Tian et al. [103] | Policy-based, BC, DDPG | vehicle kinematic | continuous steering angle & vehicle speed | path tracking | Carsim/Simulink\n",
    "Huang et al. [104] | Policy-based, BC, AC | BEV images | continuous target speed & discrete lane change action | unprotected left turn, roundabout | SMARTS\n",
    "Wu et al. [105] | Policy-based, PHIL-TD3 | BEV semantic graph | continuous steering angle & accelerating | left-turn, congestion | CARLA\n",
    "Chen et al. [107] | HRL, AC, DQN | front cam | trajectory points | lane change | TORCS\n",
    "Shi et al. [108] | HRL, DQN | relative distance & velocity | discrete lane change action & continuous acceleration | lane change | Self-made environment\n",
    "Li et al. [109] | HRL, DQN | scenario state | discrete speed & steering angle | INTERACTION dataset | OpenAI GYM toolkit\n",
    "Duan et al. [110] | HRL | policy-specific dynamics | discrete speed & steering increment | lane change | Highway environment\n",
    "Lu et al. [111] | HRL, USP-KLSPI | 14-DOF dynamics | discrete speed & steering action | lane merging | Matlab\n",
    "Gao et al. [112] | HRL, DDPG, CNN | BEV perception data, HD-Map | continuous speed & steering angle | navigation | Real-world HD-maps\n",
    "Kaushik et al. [116] | MARL, DDPG | continuous ego state, LIDAR | continuous speed & steering angle | highway navigation | TORCS\n",
    "Wang et al. [117] | MARL, PPO | relative distance & velocity | continuous acceleration | road networks | Flow\n",
    "Zhou et al. [118] | MARL, MA2C | relative distance & velocity | discrete acceleration | lane change action | Highway-env\n",
    "Chen et al. [119] | MARL, MA2C | relative distance & velocity | discrete acceleration & lane change action | lane merging | Highway-env\n",
    "Han et al. [120] | MARL, Reward Reallocation | front cam, LIDAR, vehicle kinematic | discrete lane change action | mixed traffic | CARLA\n",
    "Peng et al [121] | MARL, CoPO | continuous ego state & LIDAR | continuous acceleration & steering angle values | multi scenarios | MetaDrive\n",
    "\n",
    "#### 基于价值的强化学习\n",
    "&emsp;&emsp;基于价值的方法试图估计给定状态下不同行动的价值，并根据在一个状态下采取行动所能获得的预期回报，为每个行动学着分配一个值。智能体学着将奖励，与环境中状态和采取的行动相关联，并利用这些信息做出最佳决策[78]。  \n",
    "&emsp;&emsp;在基于价值的方法中，Q-Learning[79]是最著名的。在端到端规划中实现Q-Learning的框架如图6所示。Mnih等人[80]通过基于Q-Learning的方法提出了第一个深度学习模型，该方法直接从图像中学习，并输出控制信号。此外，Wolf等人[81]将Q-Learning方法引入智能汽车领域，他们在Gazebo模拟器[82]中定义了五种不同的驾驶maneuver，车辆根据图像信息选择相应的maneuver。为了缓解高维感知输入稳定性差的问题，条件DQN[83]方法被提出，该方法利用去模糊算法来增强不同运动命令的预测稳定性。这个模型在特定场景中实现了与人类驾驶相当的性能。  \n",
    "&emsp;&emsp;为了在特定场景下对智能汽车执行高级决策，Alizadeh等人[84]训练一个 结合 DNN的DQN 智能体，并输出两个离散动作。 自我车辆的安全性和敏捷性可以在途中被平衡，表明RL 智能体可以学习适应性行为。 此外，Ronecker 等人[85]针对高速公路场景中的智能汽车，结合控制理论和深度Q网络，提出了一种更安全的导航方法。该网络经过模拟训练，做出中央决策，为轨迹规划提供目标，这表明基于价值的强化学习可以在高速公路交通场景中说生成高效并安全的驾驶行为。  \n",
    "&emsp;&emsp;端到端自动驾驶的安全性也引起了极大的担忧。约束策略优化（CPO）[86]是一种用于约束强化学习的开创性通用策略开发算法，保证在每次迭代时满足近似约束。在此基础上，[87]和[88]提出了Safety Gym基准套件，并在约束条件下验证了几种约束深度强化学习算法。Li等人[89]将风险意识算法引入DRL框架，针对变道任务，学习具有最小预期风险的风险意识驾驶决策策略。Chow等人[90]提出了安全策略优化算法，该算法采用基于Lyapunov的方法[91]来解决CMDP问题。此外，Yang等人[92]针对离线状态约束场景，构建了一种无模型安全强化学习算法，它集成策略和neural barrier certificate learning。Mo等人[93]针对高速公路场景中超车子任务，利用蒙特卡洛树搜索减少不安全行为。  \n",
    "#### <font color=\"red\">基于策略的强化学习</font>\n",
    "&emsp;&emsp;**基于价值的方法仅限于提供离散命令**。然而，自动驾驶是一个连续的过程，在更精细的水平上不间断时间范围内的连续命令更容易受控。因此，连续方法更适用于车辆控制。基于策略的方法在高维行动空间中输出连续控制命令方面，拥有高天花板潜力。基于策略的方法比基于价值的方法表现出更好的收敛性和探索性。  \n",
    "&emsp;&emsp;在现实世界的智能汽车上执行强化学习是一项具有挑战性的任务。Kendall等人[94]在真实的智能汽车上实现了深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）[95]算法，在车上执行所有探索和优化，如图7所示。单色图像是唯一的输入，智能体学习车道跟随策略，并在250米的道路测试中达到人类水平的性能。这项工作标志着在全尺寸自动驾驶汽车上首次应用深度强化学习。为了进一步提高驾驶安全性和舒适性，Wang等人[96]提出了一种基于专家变道策略的创新方法。这种方法可以在单车或多车上执行，实现平滑变道，而无需V2X通信支持。  \n",
    "&emsp;&emsp;为了减轻拥堵道路上自动驾驶的挑战，Saxena等人[97]采用近端策略优化（proximal policy optimization，PPO）算法[98]来学习连续运动规划空间中的控制策略。他们的模型隐含地模拟了与其他车辆的交互，以避免碰撞并提高乘客的舒适度。在这项工作的基础上，Ye等人[99]利用PPO在真实的高速公路场景中学习自动变道策略。以自车及其周围的车辆状态作为输入，智能体学习避免碰撞，并以平稳的方式驾驶。其他几项研究[100]、[101]也证明了基于PPO的强化学习算法在端到端自动驾驶策略学习中的有效性。  \n",
    "&emsp;&emsp;强化学习从头开始训练策略通常耗时且困难。将强化学习与其他方法结合，例如，模仿学习（IL）和curriculum学习，可能是一种可行的解决方案。Liang等人[102]将IL和DDPG结合在一起，以缓解在探索连续空间时的低效问题，引入了一种可调节的门控机制来选择性地激活四个不同的控制信号，这使得该模型可以由中央控制信号控制。Tian等人[103]利用从专家经验中学习的强化学习方法，实现轨迹跟踪任务，该任务分两步进行训练，先采用[63]的IL方法，然后用连续的、确定性的、无模型的强化学习算法进一步微调该方法。  \n",
    "&emsp;&emsp;为了解决强化学习方法的学习效率限制，Huang等人[104]设计了一种新方法，将人类先验知识整合到强化学习方法中。当面对自动驾驶的长尾问题时，许多研究人员将视角转到专家经验的开发。Wu等人[105]提出了一种基于人类指导的强化学习方法，该方法利用一种新颖的优先经验回放机制，提高强化学习算法在极端场景的效率和性能，所提出方法的框架如图8所示。该方法在两个挑战性的自动驾驶任务中得到了验证，并取得了具有竞争力的结果。  \n",
    "#### <font color=\"red\">分层强化学习</font>\n",
    "&emsp;&emsp;强化学习方法在各个领域都表现出了巨大的前景，然而，这些方法经常因训练困难而受到批评。特别是在自动驾驶领域，非平稳的场景和高维的输入数据会导致无法忍受的训练时间和资源使用[106]。分层强化学习（Hierarchical reinforcement learning，HRL）将整个问题分解为子任务的层次结构，每个子任务都有自己的目标和策略。子任务以分层方式组织，较高级别的子任务为较低级别的子任务提供情景和指导。这种分层组织允许智能体专注于较小的子问题，降低学习问题的复杂度，使其更易于处理。  \n",
    "&emsp;&emsp;**在强制执行变道任务时，Chen等人[107]提出了一种双层方法。高层网络学习策略，以决定是否执行变道动作，而低层网络学习策略，用于执行所选择的命令**。[108]和[109]基于[107]提出了两阶段HRL方法，其中[108]需要采用纯追踪来跟踪输出轨迹点，[115]集成自车的位置、速度和航向，以进一步提高低级别控制器的性能。所有这些提出的方法为开发鲁棒和安全的自动驾驶系统提供了一个有希望的解决方案。  \n",
    "&emsp;&emsp;HRL的通用性是一个研究热点。Lu等人[111]针对复杂动态交通场景，提出了一种适用于自主决策和运动规划的HRL方法，如图9所示。该方法由高层和低层规划层组成，高层利用基于核的最小二乘策略迭代算法，及不均匀采样和pooling策略（USPKLSPI），求解决策问题。Duan等人[110]将整个导航任务分为三个模型。训练主策略网络以选择合适的驾驶任务，该策略极大地提高了模型的通用性和有效性。为了进一步提高复杂场景的决策质量，在[110]的基础上，**Cola HRL[112]被提出，该方法包含三个主要组成部分：高级规划器、低级控制器和状态空间的连续网格表示**。规划器和控制器都使用状态空间生成高质量的决策。结果表明，Cola-HRL在大多数场景下优于其他SOTA方法。  \n",
    "#### <font color=\"red\">多智能体强化学习</font>\n",
    "&emsp;&emsp;在真实场景中，通常存在不同的交通参与者，他们的交互会对彼此的策略产生重大影响[113]。在单智能体系统中，其他参与者的行为通常由基于预定义的规则控制，智能体的预测行为可能会过度拟合其他参与者，进而导致比多智能体系统更具确定性的策略[114]。**多智能体强化学习（MARL）旨在学习环境中多智能体的决策策略。一种流行的MARL建模方法是离散的部分可观测马尔可夫决策过程（DEC-POMDP）**。然而，状态空间随着智能体的数量指数增长，使训练多智能体系统（MAS）更具挑战性和速度更慢[115]。  \n",
    "&emsp;&emsp;为了减少“维度爆炸”的影响，一些有效的学习方案被提出。Kaushik等人[116]使用一个简单的参数共享DDPG来训练智能体执行两个不同的任务。通过将任务作为命令注入观测空间，同一代理可以竞争或合作行动。Wang等人[117]在三种场景中训练自主智能体：环形网络、八字形网络和具有各种场景的迷你城市。每个智能体之间的地图信息共享被集成在 PPO方法中，用于连续动作的生成，并允许在一定范围内进行车辆通信。  \n",
    "&emsp;&emsp;尽管强化学习在变道决策方面已被广泛的研究，但这些研究主要集中在单智能体系统。MARL方法提供了多车辆控制的全局视角。Zhou等人[118]在与人类驾驶汽车共存的混合交通高速公路场景中，构建了多自动驾驶汽车的变道决策。除了简单的任务外，MARL方法在复杂场景中，求解决策和规划问题具有巨大的潜力。Chen等人[119]在临界时间汇入高速公路场景中，训练多智能体避免碰撞。智能体观察周围车辆的位置和速度，然后选择相应的行动。  \n",
    "&emsp;&emsp;信用分配对于多智能体合作场景中的策略学习至关重要。Han等人[120]引入了一种有效奖励再分配机制，使用考虑Shapley值奖励再分配的合作策略学习算法，激励智能汽车之间的稳定合作。该机制的实验结果表明，联网自动驾驶汽车的平均episode系统奖励显著提高。Peng等人[121]没有在智能体之间重新分配奖励，而是将社会价值取向的环测量纳入自驱动粒子（SDP）系统，这是MAS的一个类别。因为SDP系统中的每个组成智能体都是利己主义的，智能体之间的关系是不断变化的。所提出的协调策略优化（CoPO）方法，在一定距离内，在智能体与其相邻车辆之间施行局部协调，如图10所示。实验表明，所提出的方法在成功率、安全性和效率三个主要指标上优于MARL基线。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634b90a",
   "metadata": {},
   "source": [
    "### 并行学习\n",
    "&emsp;&emsp;自动驾驶中的规划方法受到若干挑战的限制。**pipeline规划方法结合了许多人为定制的启发式方法，导致低效的计算和较差的泛化。模仿学习（IL）方法需要大量且分布多样的专家轨迹，而强化学习（RL）方法需要大量的计算资源**。因此，这些限制的存在阻碍了自动驾驶的广泛实施。  \n",
    "&emsp;&emsp;为了应对规划方法中的各种问题，虚拟与物理的交互提供了一种行之有效的解决方案[129]。基于智能控制的网络物理系统（CPS）可以促进物理和网络空间之间的交互和集成，但未考虑系统中的人为和社会因素。为了解决这个问题，许多研究员在CPS中加入社会因素和人工信息，形成网络-物理-社会系统（CPSS）。在CPSS中，“C”代表两个维度：现实世界的信息系统和由软件定义的虚拟人工系统。“P”指的是传统的现实系统。“S”不仅包括人类社会系统，还包括基于现实世界的人工系统。  \n",
    "&emsp;&emsp;CPSS能够使虚拟和现实系统相互交互、反馈和促进。现实系统为人工系统的构建和标定提供有价值的数据集，而人工系统则指导和支持现实系统的运行，从而实现自我进化。由于虚拟与现实的交互优势，CPSS为端到端自动驾驶提供了一种新的验证方法。  \n",
    "&emsp;&emsp;Fei-Yue Wang[122]在2004年基于CPSS提出了并行系统理论的概念，如图11所示，其核心概念是ACP方法，它是人工社会（A）、计算实验（C）和并行执行（P）的有机结合。在过去的二十年里，并行系统理论的研究体系通过大量的实践得到丰富和完善，如并行智能[131]、并行控制[132, 133]、并行管理[134]、并行运输[135]、并行驾驶[126, 136]、并行跟踪[137]、并行测试[128]、并行视觉[125]。关于本节中提出的方法的总结如[表3](#table3)所示。  \n",
    "\n",
    "表3<span id='table3'></span>\n",
    "\n",
    "Method | Year | Detail\n",
    ":- | :- | :-\n",
    "CPS | 1990s | 基于大数据、物联网、大计算，提出一种计算、通信与控制（3C）有机融合并深度协同的多维智能技术框架\n",
    "CPSS [123] | 2000 | 将社交信号和关系整合到CPS，利用社会网络的人、数据和信息，突破现实世界的各种局限\n",
    "Parallel System Theory [122] | 2004 | 耦合人工社会（A）、计算实验（C）和并行执行（P），为复杂系统的控制和管理提供有效工具\n",
    "Parallel Learning [124] | 2017 | 提出了一种新的机器学习理论框架，即并行学习，它融合和继承了现有各种机器学习理论的许多元素。\n",
    "Parallel Vision [125] | 2017 | 将并行系统理论引入计算机视觉领域，构建了复杂驾驶场景下感知和识别的新研究方法\n",
    "Parallel Driving [126] | 2019 | 构建一个自动驾驶高级统一框架，包括运营管理、在线状态管理和紧急脱困\n",
    "Parallel Planning [127] | 2019 | 构建一种集成卷积神经网络和长短期记忆模型的深度规划方法，以提高智能汽车的规划模型的泛化性和鲁棒性\n",
    "Parallel Testing [128] | 2019 | 提出一个闭环测试框架，在更具挑战性的场景实施，以加速自动驾驶汽车的评估和开发\n",
    "\n",
    "&emsp;&emsp;为了进一步扩展神经网络的学习能力，并应对模仿学习和强化学习的挑战，Li等人[124]基于并行系统理论提出了并行学习的基本框架，如图12所示。**在行动阶段，并行学习[124]遵循强化学习的范式，采用状态转移来表示模型的运动，从大数据学习，并将学习到的策略存储在状态转移函数中**。 值得注意的是，并行学习利用计算实验来细化策略。 通过特征提取的方法，小知识可以被应用到特定的场景或任务中，并被用于并行控制。这里的“小”指的是特殊问题的特定智能知识，而不是知识的量级。  \n",
    "&emsp;&emsp;一种基于并行学习的创新训练方法[124]为完全端到端的自主堆栈提供了一种问题求解的替代方案。如图13所示，Wang等人[138]引入了一个并行驱动框架，这是ITS和智能汽车的统一方法。该框架直接连接专家轨迹和控制命令，计算特定场景的最佳策略。从现实场景中收集大量的专家轨迹，并使用神经网络学习所有轨迹，该网络的输入和输出分别是目标状态和控制信号。从并行学习的角度来看，这是一个自我标记的过程，该过程显著缓解了端到端方法的数据饥饿。  \n",
    "&emsp;&emsp;为了处理来自人工系统和计算实验的融合数据，提出了一种并行强化学习（PRL）的新理论，它结合了并行学习和深度强化学习方法。Liu等人[126]集成digital quadruplets与并行驱动。该框架定义了物理车辆、描述性车辆、预测性车辆和规定性车辆。基于digital quadruplets的描述，三辆虚拟汽车可以被定义为实体汽车的三个“守护天使”，在复杂场景中扮演不同的角色，使智能汽车更安全、更可靠。  \n",
    "&emsp;&emsp;规划是自动驾驶最重要的组成部分之一。作为并行驱动的具体实例，Chen等人[126, 138]提出了一个端到端规划的并行规划框架，该框架构建了两种定制方法以求解特定场景中的应急规划问题。对于数据不足的问题，并行规划利用人工交通场景，基于来自现实预训练的知识生成专家轨迹，如图14所示。对于非鲁棒性问题，并行规划利用变分自动编码器（VAE）和生成对抗性网络（GAN），学习从人工交通场景中生成的虚拟紧急情况。对于学习低效的问题，并行规划从虚拟和现实场景中学习策略，最终决策由现实观察的分析确定。当紧急情况发生时，并行规划能够做出合理的决策，而无需繁重的计算负担。  \n",
    "&emsp;&emsp;并行系统理论为复杂系统的控制和管理提供了有效的工具，特别是在自主控制领域，并行驱动有效地缓解了端到端规划模型的数据缺失、学习低效和鲁棒性差。  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdfca9",
   "metadata": {},
   "source": [
    "## 实验平台\n",
    "&emsp;&emsp;在现实系统中测试智能驾驶通常会带来潜在的致命安全风险。因此，自动驾驶算法通常在人工系统中使用开源数据集和仿真平台进行评估[154]。  \n",
    "### 数据集\n",
    "&emsp;&emsp;端到端方法利用广泛可用的大规模人类驾驶数据集进行训练，以接近人类标准。因此，训练过程需要大量来自驾驶场景的数据。数据集的量级、丰度和分布直接影响训练模型的安全性、鲁棒性和泛化性。尽管构建和组装智能汽车的新数据集非常耗时，但许多通用且有影响力的数据集可用于研究，例如Comma.ai[140]、Bdd100K[146]、A2D2[147]、Automine[148]、DriverTruth[152]和Sups[151]，大多数著名的数据集如[表4](#table4)所示。\n",
    "\n",
    "表4<span id='table4'></span>\n",
    "\n",
    "Dataset | Year | Sensors | Scenarios\n",
    ":- | :- | :- | :-\n",
    "KITTI [139] | 2013 | 4 cameras; 1 LiDAR | City; Countryside; Highway\n",
    "Comma.ai [140] | 2016 | 1 monocular camera; 1 point grey camera | Highway scenarios\n",
    "Oxford RobotCar [141] | 2016 | 6 Cameras; 3 LiDARS; Speed; GPS; INS | City; Contain weather changed\n",
    "Mapillary Vistas [142] | 2017 | Image devices | Street Scenarios\n",
    "nuScenes [143] | 2019 | 6 Cameras; 5 Radars; 1 LiDAR | Street Scenarios\n",
    "ApolloScape [144] | 2019 | 2 Cameras; 2 LiDAR; GPS; IMU | Street Scenarios\n",
    "Waymo Open Dataset [145] | 2019 | 5 Cameras; 5LiDAR | 1150 Street Scenarios\n",
    "BDD100K [146] | 2020 | 1 Camera; GPS; IMU | Street scenarios in 4 cities\n",
    "A2D2 [147] | 2020 | 6 Cameras; 5 LiDAR; GPS; IMU | $360^{\\circ}$ Street Scenarios\n",
    "Automine [148] | 2021 | 2 Cameras; 1 LiDAR; GPS; IMU | The first open-pit mine dataset\n",
    "AI4MARS [149] | 2021 | 2 Cameras | The first large-scale dataset in Mars\n",
    "SODA10M [150] | 2021 | 1 Camera | City Scenarios in 31 cities with all kinds of weathers\n",
    "SUPS [151] | 2022 | 6 Cameras; 1 LiDAR; GPS; IMU | Underground parking scenarios\n",
    "DRIVERTRUTH [152] | 2022 | 1 Camera; 1 LiDAR; GPS; IMU; Control signal | City Scenarios based-on CARLA\n",
    "ROAD [153] | 2023 | 1 Camera | Scenarios in [141] for road event detection\n",
    "\n",
    "&emsp;&emsp;KITTI [139] 是该领域的先驱，也是最著名的自动驾驶数据集。 由于其良好的任务缩放，KITTI 现在涵盖了大范围的白天感知任务，例如对象检测、场景流、深度估计、跟踪等。  \n",
    "&emsp;&emsp;Comma.ai [140]通过额外收集定位信息和控制信号来丰富数据的多样性，因此它可以用于更多任务，例如定位和规划。  \n",
    "&emsp;&emsp;BDD100K [146] 和 SODA10M [150] 通过构建大规模模拟场景来缓解多样性和数量问题，它们都收集了超过 31 个城市的各种天气条件下的城市场景，它们还带有丰富的标签集：场景标记、对象边界框、车道标记、可行驶区域、全帧语义和实例分割、多对象跟踪。  \n",
    "&emsp;&emsp;A2D2 [147] 是一个商业级数据集，非常适合各种感知任务，弥合了公开数据集缺乏全方位车辆信息的空白。与之前的数据集相比，它通过5个激光雷达提供了$360^{\\circ}$的点云感知场，实现了自动驾驶的全场景感知。  \n",
    "&emsp;&emsp;下列数据集提供了不同于结构化场景的交通场景。Automine [148] 为智能汽车构建了开创性的露天矿场数据集，包括从6个露天矿场收集的 18 小时的运输视频和定位信息。露天矿场的独特特征，如崎岖不平的地形、强光和大量灰尘，带来巨大挑战。 Automine 是填补露天矿场数据集空白的宝贵资源，支持了自主采矿技术的进步。AI4MARS[149]提出了另一个有趣的大规模数据集，它包含35,000张火星表面的语义分割完整图像。  \n",
    "&emsp;&emsp;目前，数据集几乎涵盖了自动驾驶的所有任务，并且在促进智能车辆算法的训练和验证方面发挥着越来越重要的作用。这些支持为实现自动驾驶奠定了必要的基础。  \n",
    "### 仿真平台\n",
    "&emsp;&emsp;在现实场景中测试自动驾驶算法往往伴随着巨大的潜在风险，仿真测试提供了一种验证算法的智能方法，由于其低成本和高安全性，可以加快测试。  \n",
    "&emsp;&emsp;许多自动驾驶仿真平台都已开发出开源代码和协议，可用于自动驾驶的算法测试。SUMO[155]是一个开源的微观交通模拟平台，由德国航空航天中心开发，为大规模交通算法提供了一个强有力的验证平台。它配备了一个精心设计的接口，支持广泛的数据格式。由于其卓越的功能，SUMO已成为最早和应用最广泛的模拟器之一。此外，Apollo[144]和Autoware[156]不仅为验证算法提供了一个模拟平台，而且为每个任务配备了开源算法，为开发人员提供了完整的开发-验证-部署链。  \n",
    "&emsp;&emsp;在单车自动驾驶方法的背景下，CARLA[157]提供了一个合适的答案。它是一个用于城市自动驾驶场景的开源模拟器，有助于潜在的城市自动驾驶系统的开发、训练和验证。  \n",
    "&emsp;&emsp;在多车辆交互方法领域，TORCS[158]提供了一个开放式赛车模拟器，具有50多种车辆模型和20多条赛道。此外，它能够同时与50辆赛车竞赛，这使得它成为该领域研究的宝贵工具。MetaDrive[159]提出了一个开源平台，以支持机器自主的通用强化学习算法研究。它具有高度的组合性，能够通过程序生成和现实数据导入生成无穷的驾驶场景。其他仿真平台及其相关描述如[表5](#table5)所示。  \n",
    "\n",
    "表5<span id='table5'></span>\n",
    "\n",
    "Platform | Latest Version | Description\n",
    ":- | :- | :-\n",
    "PTV Vissim | V2023 | Traffic simulation platform focused on complex intersection design and active traffic management.\n",
    "VTD | V2.2 (19.01) | Provides a complete bottom-up simulation platform, including ADAS and automation systems.\n",
    "SUMO [155] | V1.15.0 (22.11) | Provides a purely microscopic traffic model that can be defined to customize each vehicle.\n",
    "TORCS | V1.3.8 (17.03) | Support for running a large number of agents at the same time, allowing for scheduling functions in dense vehicle areas.\n",
    "SVL Simulator [160] | V2021.3 (21.05) | Enables developers to simulate billions of miles and arbitrary corner cases to accelerate algorithm development and system integration.\n",
    "V-Rep | V3.6.2 (19.01) | With a driving actions assessment function, which indicates the agent behavior based on the result.\n",
    "CarMaker | V10.0 (21.10) | Specifically designed for the development and seamless testing of cars and light-duty vehicles in all development stages.\n",
    "CARLA [157] | V0.9.13 (21.11) | Various city maps are provided for autonomous driving algorithms, as well as support for customized sensor types and weather conditions.\n",
    "AriSim [161] | V1.8.1 (22.06) | The capability to quickly complete autonomous driving tests, and build various scenarios (urban, countryside, highway, field, etc.)\n",
    "Apollo [144] | V8.0 (22.12) | Support for learning and validation of single and multi-vehicle autonomous driving algorithms on urban scenarios.\n",
    "Autoware [156] | V1.11.0 (21.05) | An open-source autonomous driving platform, which include all component of autonomous function for intelligent vehicle.\n",
    "Drive Constellation | V6.05 (22.10) | Provides a computing platform based on two different servers that can undertake large-scale vehicle data interaction services.\n",
    "MetaDrive [159] | V0.2.6.0 (22.11) | A wide range of road segments are available, which can be customized to generate a variety of complex scenarios, more suitable for reinforcement learning.\n",
    "\n",
    "### 物理平台\n",
    "&emsp;&emsp;随着计算机计算能力的提高，仿真测试越来越能够满足各种场景的测试需求，并已被证明可以有效解决与此类系统相关的长尾问题。然而，模拟器中使用的预训练模型，通常需要在现实世界实施之前进行微调。此外，虽然仿真测试可以覆盖广泛的场景，但它无法解释所有的极端案例。因此，一个专业且安全的半开放式自动驾驶验证站点是必不可少的[162]。  \n",
    "&emsp;&emsp;自动驾驶技术在过去几十年中取得长足发展，一些国家出台政策，允许公共道路上的无人驾驶出租车测试。在美国，从2022年起，Waymo被允许在旧金山街头测试无人驾驶出租车。Nuro最近在亚利桑那州、加利福尼亚州和得克萨斯州，开始部署自动送货车。在英国，Aurigo在伯明翰机场开展一项自动接驳的试验。Wayve被授权在五个城市之间进行长距离自动驾驶汽车测试。在中国，自动驾驶的商业化正在迅速推进，Apollo、Pony和Momenta等公司已经在几个城市投放了自动驾驶汽车。此外，Waytous正在非结构化和封闭场景中开展无人运输，并为多个露天矿场提供无人驾驶解决方案。  \n",
    "## 挑战与未来方向\n",
    "&emsp;&emsp;自动驾驶已经取得了长足的进步，多城市半开放道路的成功验证证明这一点。然而，由于需要克服重重障碍和迫在眉睫的挑战，其完整的商业部署尚未实现。  \n",
    "### 挑战\n",
    "&emsp;&emsp;智能汽车的挑战总结如下：  \n",
    "&emsp;&emsp;**感知的限制**：大多数自动驾驶框架严重依赖感知结果，但大多数传感器受其固有约束的限制。视觉传感器易受视野和天气的影响，在背光和强光照射下效果较差。感知结果往往存在局部感知问题，因此被障碍物掩盖的潜在危险可能会被忽视。这些缺点给自动驾驶带来安全挑战。  \n",
    "&emsp;&emsp;**规划的限制**：pipeline和端到端规划都有固有的局限性，确保在不确定和复杂场景下生成高质量结果是不可或缺的研究目标。  \n",
    "&emsp;&emsp;**安全的限制**：自动驾驶系统的黑客攻击事件越来越多，即使轻微的中断也可能引发决策的重大偏差。因此，自动驾驶技术的大规模成功部署需要强有力的措施，以对抗黑客攻击。  \n",
    "&emsp;&emsp;**数据集的限制**：仿真数据集有助于模型训练，而仿真环境中训练好的模型往往不能直接转移到现实。因此，弥补虚拟和现实数据之间的差距是当务之急的研究途径[163]。  \n",
    "### 未来方向\n",
    "&emsp;&emsp;目前，规划方法难以处理所有的复杂场景，模型也受到安全性、通用性和可解释性的限制。未来的研究趋势包括：  \n",
    "1. 面对感知的限制。许多研究人员试图将认知纳入感知层。通过利用人类的认知能力，可能克服自动驾驶的挑战。  \n",
    "2. 解决端到端方法的不可解释性问题。许多研究人员在潜在层生成可解释的中间表示，以增强可解释性。端到端方法用这些表示的探索代表了智能汽车的研究方向。  \n",
    "3. 管理智能汽车的黑客攻击问题，目前的防御措施已被证明不足以抵御**SOTA**攻击，针对此类攻击的强力防御技术的开发具有重要的研究意义。  \n",
    "4. 面对复杂情况下的决策挑战。将人类的认知能力融入自动驾驶，以及场景特征的全面了解，是克服现有局限的有效方法。  \n",
    "5. 考虑到规划方法的鲁棒性和通用性所带来的挑战，ChatCPT[164]中训练好的大模型在求解复杂问题方面表现出超越人类水平的能力。这在自动驾驶领域也成立，其中一个有前途的未来方向是合理化大型模型的应用。  \n",
    "6. 面对从虚拟到现实的数据集迁移的挑战，并行系统理论的描述原理[165]可能是一种有效的解决方案。通过描述原理耦合数据的两种类型，生成一个反馈回路，从而实现循环自优化。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6a82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "224.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
